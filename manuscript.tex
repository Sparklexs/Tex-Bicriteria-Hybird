% This is "sig-alternate.tex" V2.1 April 2013
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate-05-2015}
\usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
\usepackage{amsmath,amssymb}
\usepackage{multirow,booktabs,makecell,color,colortbl}
\usepackage{color}
\usepackage{balance}
\graphicspath{{./pdf/}}

\begin{document}

% Copyright
\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\doi{10.475/123_4}

% ISBN
\isbn{123-4567-24-567/08/06}

%Conference
\conferenceinfo{WSDM '17}{February 6-10, 2017, Cambridge, UK}

\acmPrice{\$15.00}

%
% --- Author Metadata here ---
%\conferenceinfo{WSDM}{'17 Cambridge, UK}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{A Flexible Space-time Tradeoff on Hybrid Index with Bicriteria Optimization}

\numberofauthors{4} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
%AUTOR
%%% 1st. author
%\alignauthor Xingshen Song\\
%\affaddr{College of Computer}\\
%\affaddr{National University of Defense Technology}\\
%\affaddr{Changsha, China}\\
%\email{songxingshen@nudt.edu}
%% 2nd. author
%\alignauthor Yuexiang Yang\\
%\affaddr{College of Computer}\\
%\affaddr{National University of Defense Technology}\\
%\affaddr{Changsha, China}\\
%\email{yyx@nudt.edu}
%% 3rd. author
%\alignauthor Kun Jiang\\
%\affaddr{School of Electronic and Information Engineering}\\
%\affaddr{Xi'an Jiaotong University}\\
%\affaddr{Xi'an, China}\\
%\email{jk\_365@126.com}
%\and  % use '\and' if you need 'another row' of author names
%% 4th. author
%\alignauthor Yu Jiang\\
%\affaddr{College of Computer}\\
%\affaddr{National University of Defense Technology}\\
%\affaddr{Changsha, China}\\
%\email{jiangyu@nudt.edu}
%AUTHOR
}
\date{12 July 2016}

\maketitle
\begin{abstract}
Inverted Indexes are widely adopted in the vast majority of information systems.
Growing requirement for efficient query processing has motivated various compression techniques with different space-time characteristics.
While a single encoder yields a relatively stable point in the space-time tradeoff curve, flexibly transforming its characteristic along the curve to fit different IR tasks can be a better way to prepare the index.
Recent Research come out with an idea of integrating different encoders within the same index, namely, exploiting access skewness by compressing frequently accessed regions with faster encoders and rarely accessed regions with succinct encoders.
Thus improving the efficiency while minimizing the compressed size.

However, these methods are either at a coarse granularity or algorithmically complex in practice.
To tackle these issues, we introduce the notion of \textit{Bicriteria Compression} to formalize the problem of optimally trading the compressed size and query processing time for inverted index.
We also adopt a Lagrangian relaxation algorithm to solve this problem by reducing it to a knapsack-type problem, which works in $ O(n\log n) $ time and $ O(n) $ space, with a negligible additive approximation.
We perform an extensive experiment to show that, given bounded time/space budget, our method can optimally trading one for another with more efficient indexing and query performance.
\end{abstract}

\begin{CCSXML}
	<ccs2012>
	<concept>
	<concept_id>10002951.10002952.10002971.10003451.10002975</concept_id>
	<concept_desc>Information systems~Data compression</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	<concept>
	<concept_id>10002951.10003317.10003365.10003367</concept_id>
	<concept_desc>Information systems~Search index compression</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Data compression}
\ccsdesc[500]{Information systems~Search index compression}

\printccsdesc

\keywords{Inverted Index; Bicriteria Compression; Lagrangian Relaxation}

\section{Introduction}\label{sec:intro}
Indexing is essential to efficiently manage growing data and perform quick queries in many information retrieval systems.
The use of inverted Indexes in web search engine is probably one of the most successful examples in IR.
Despite of the fact that it is a relatively old yet simple data structure, inverted index is extensible with supplementary information to answer different queries within much less space occupied.
Compression has been one of the most active field of inverted index research in the last decades, for it not only reduces the space occupancy, but also accelerates data transfer in I/O.
See \cite{catena2014inverted,trotman2014compression,zobel2006inverted} for a detailed survey.

Typical inverted index is composed of two parts: \textit{lexicon} and \textit{posting lists}.
The inverted index compression we are talking about is usually concentrated in the compression of posting lists.
However, there exists a divergence between compressed size and decompression time,
since pursuing the minimization of size will undoubtedly impair decoding.
Encoders with different space-time treadoffs keep coming out to settle for requirements of various user scenarios \cite{lemire2015decoding,ottaviano2015optimal,ottaviano2014partitioned,petri2014score,silvestri2010vsencoding}.
Among these techniques, splitting posting list into \textit{blocks} (or \textit{chunks}) has been demonstrate to be an efficient way and widely adopted by many encoders.
For the reason that by keeping blocks separately accessible we can expect faster decoding speed and hierarchical structure.
In addition, variable-sized blocks can utilize the clustering property of list to beat the entropy \cite{moffat2000binary,silvestri2010vsencoding}.
Nonetheless, designers of information systems have to carefully choose one of these encoders to satisfy different IR tasks: whether to achieve optimal compression ratio/decoding speed, sacrificing the other one; or to balance them by adopting one proper encoder with finely tuned space-time tradeoff.

Since a single encoder has its own stable performance in the space-time tradeoff curve, it would be natural to ask for an encoder, which can trade the space occupancy and decoding time flexibly, to fit in different system requirement and device context.
An intuitive way to solve this problem is by combing different encoders into one index.
Namely, compressing the frequently accessed parts using a faster but less space-efficient encoder, and other parts using a slower encoder that gives better compression.

This idea is first adopted in \cite{yan2009inverted}.
However, the procedure of choosing encoders works at a posting list granularity, which is too coarse to obtain a flexible space-time tradeoff.
They also fail to explore complexity of the algorithm as it works on a empirical foundation.
Giuseppe et al. \cite{ottaviano2015optimal} explicitly define the Space-Constrained Expected Time Minimization problem: given a dataset $ D $ and an upper bound $ \cal S $ on its compressed size, the goal is to choose one proper encoder from an encoder set $ \cal E $ for each block of the index.
Thus minimizing the expected query processing time, provided that it can be compressed within $ \cal S $ space.
They then propose a linear time solution for this problem, which is quite straight forward.
Under two predefined dominance criteria, their algorithm works by enumerating all the advisable compression methods, then sorting them in an non increasing order by the time/space gain ratio of each advisable encoder on each block.
After this, a sequential scan and replace is able to find all the optional solutions for any possible budget constraint $ \cal S $.
Given the fact that each posting list is divided into fixed-sized blocks, whose total number is $ n $, and a set of $ k $ encoders, the algorithm finishes in $ \Theta(nk\log(nk)) $ time and $ O(nk) $ space.

By refining the granularity to block level and the carefully designed algorithm, they are able to find a solution with minimal query time.
Also, the approximation is additive, which is at most a block size larger than $ \cal S $.
However, the algorithm takes too much space to sort the compression options that it can be hardly fit into memory; second, since the sorting and sequential scan cannot be accelerated by parallel, the whole procedure is delay by them somehow; third, their solution is based on assumption that the posting lists are split into fixed-sized blocks, otherwise the compression options are hardly to sort and might take much more space. 

We argue that global ordering of the compression options for all the blocks is not a good idea, since it will lead to the above three shortcomings.
In this paper, we try to solve this problem from another perspective.
The main contributions are listed below:
\begin{enumerate}
	\item We introduce the conception of \textit{Bicriteria Compression} \cite{farruggia2014bicriteria} to extend the Space-Constrained Expected Time Minimization problem.
	Except the original one, it models another dual problem: we could exchange the role of space and time by asking for the minimal compressed size, provided an upper bound $ T $ in its average query processing time or decompression time.
	By solving the \textit{Bicriteria Compression} problem we can expect the compression method to be more universal for different scenarios.
	\item We reduce the problem into a Resource Constrained Shortest Path problem \cite{mehlhorn2000resource} over a weighted DAG $ \cal G $, whose edges contain two kinds of penalties: space cost and time weight.
	Thus, the problem can be formalized using Lagrangian relaxation, we use the algorithm introduced in \cite{farruggia2014bicriteria} and \cite{handler1980dual} to settle in $ O(n\log n) $ time and $ O(n) $ space, while keeping the same additive approximation guarantee.
	\item Different from \cite{ottaviano2015optimal} which is a greedy algorithm, our method amortizes the expected budget over each posting list in a more balanced way.
	Thus we could expect for a better average performance.
	While keeping the same space and time complexity, our method is more efficient and needs less runtime memory in practice without global sorting.
	\item We execute a preliminary set of experiments on two realistic dataset, GOV2 and Common Crawl, with AOL query log.
	Analysis shows that our method is able to obtain flexible tradeoffs under bicriteria compression.
	Its compressed size and query processing time is very competitive to the hybrid index in \cite{ottaviano2015optimal}, however, with more detailed description and more strict theoretic foundation.
\end{enumerate}

\section{Preliminaries}\label{sec: preliminaries}
\subsection{Index Structure and Compression}
Given a collection of \textit{D} documents, an inverted index can be seen as a big table mapping each unique \textit{term} to a\textit{ posting list} which contains all the document identifiers (called \textit{docid}) and the number of occurrences in the document (called \textit{frequency}), and possibly other information like the positions of each occurrence within the documents (if to support phrasal and proximity matching).
The set of terms is called \textit{lexicon}, which is relatively small compared to postings.
To rank the documents in response to a query, the posting list for the terms of the query must be traversed.
There is a vast variety of literature study on structures and query processing strategies, see \cite{ding2011faster,konow2013faster,navarro2010dual} for a detailed survey.

%Different orderings of the postings in the posting list change both the algorithm for ranked retrieval and the underlying representation of the postings in the inverted index.
%Generally, there are two kinds of orderings, namely the docid-sorted and frequency-sorted, in docid-sorted index postings are sorted by docid in ascending order, allowing efficient compression and fast query processing; in frequency-sorted index postings are sorted by frequency in descending order and postings with same frequency are sorted by docid, an early termination can be reached for ranked query since the most promising postings are placed close to the start of the list.
%A fatal problem with frequency-sorted index is that unordered docids are hard to compress, commercial search engines reportedly use the former obtain very good results by \cite{dimopoulos2013optimizing}.
%The appropriate compression of docid-sorted index is the focus of our work.

To facilitate both compression and query processing, posting lists from the index are always considered separate data stream, components in each posting lists (docid, frequency and position) are stored in a non-interleaved way, and the lists of docid and position are usually sorted and transformed to the corresponding sequence of differences(or \textit{gaps}) between adjacent values.
Hence, the task of index compression is best viewed code sequences of integers of the form $x_{1},x_{2},x_{3},\ldots,x_{n}$ where $x_{i}\geqslant0$ for all $1\leqslant i\leqslant n$, and $n\geqslant1$ is the length of the sequence (\cite{anh2010index}).

Index compression is a relatively old and established topic in the literature of IR, researchers have summarized the previous work explicitly \cite{catena2014inverted,lemire2015decoding,silvestri2010vsencoding,trotman2014compression}.
The variety of compression techniques can be roughly divided into two classes, namely the \textit{integer-oriented encoders} and the \textit{list-oriented encoders}.

The integer-oriented encoders assign an unique codeword to each integer of the input list, then the compression procedure turns into a mapping or substitution from the integer space to code space.
As they compress integers without considering their neighborings, the integer-oriented encoders are also called oblivious encoders\cite{catena2014inverted}, such as \textit{unary code, Elias Gamma/Delta codes \emph{and} Golomb/Rice codes}.
Most integer-oriented encoders are hard to decode since they need bitwise operations to cross computer word boundaries, so byte/word-aligned encoders, are proposed to solve this problem, like \textit{Variable Byte} and \textit{Group Varint}, more importantly, they can be further improved by SIMD instructions of modern CPUs\cite{stepanov2011simd,trotman2014compression}.

List-oriented encoders are designed to exploit the cluster of neighboring integers, each time a fixed-sized or variable-sized group of integers is binary packed with an uniform bit width, providing equivalent compression ratio and faster decoding speed, the technique used by these encoders is called \textit{frame-of-reference}(FOR), or \textit{binary packing}\cite{delbru2010adaptive,goldstein1998compressing}.
Basically, their compression ratios are inferior to these of the first category as a batch of integers are encoded indiscriminately, and useless zeros are filled in the codeword to keep word-aligned.
However, when decoded, list-oriented encoders can obtain an entire block while the formers just decode one integer at a time.
More importantly, with the help of \textit{skip pointers} or \textit{skip list}, it is possible to step along the codewords compressed by list-oriented encoders and stop when the required number of blocks has been bypassed.
Some of these encoders are \textit{Simple Family} \cite{anh2004index,anh2005inverted,anh2006improved,anh2010index}, AFOR \cite{delbru2012searching} and \textit{Patched} FOR (PFOR, OptPFOR and FastPFOR) \cite{lemire2015decoding,yan2009inverted,zukowski2006super}.

In our scenario, we follow the settings used in \cite{ottaviano2015optimal}, that encoders in $ \cal E $ are chosen from list-oriented encoders and posting lists are uniformly split into fixed-sized blocks of 128 postings.
And those posting lists are virtually concatenated into one large list to facilitate our computation under bounded space/time budget, as we only store some statistics about each block, this will not take too much space.

\subsection{Pareto-optimal Compression}
Recently, researchers begin to use breakthroughs in succinct data structures to bear on the problems in inverted index compression.
Giuseppe et al. \cite{ottaviano2014partitioned} use pruned DAG \cite{ferragina2011optimally} to optimally partition posting list for minimal compression and convenient access.
Petri et al. \cite{petri2014score} describe an data structure combining pruned suffix tree and inverted index together to facilitate phrase-based ranking.
Our work is similar to these efforts, applying bicriteria compression to inverted index.

In our context, we are interested in optimally trading space occupancy with query processing time under specified space/time budget.
With respect to the notion of ``optimal", we introduce the concept of \textit{Pareto-Optimal Compression} to define it in a principled way: encoders achieve different points in the space-time trade-off curve, if to set the space occupancy and query processing time as two dimensions of the coordinate system.
Optimal ones are those extreme points whose performances are not worse than others in one dimension, and if to be optimized in any dimension, performances in the other two will get impaired accordingly.
Obviously there exists a set of Pareto-Optimal Compressions with different considerations.
As an extreme example, consider the two solutions: one is space-optimal compressed by Binary Interpolative Coding \cite{moffat2000binary} and the other one is time-optimal compressed by SIMD encoders.
That is, for any block of the posting list, we always choose to use either the space-efficient encoder or the time-efficient one.
However, in between these two extremes, we still have a plethora of encoders like PFOR and Simple-X.
Our goal is to move from one extreme to other by trading decompression speed for space occupancy, or vice versa.
Since there are $ k $ encoders for $ n $blocks, the key is to automatically and efficiently choose any of them for optimal tradeoffs.

\subsection{Directed Acyclic Graph}
By modeling index compression as an Directed Acyclic Graph $ \cal G $, finding the optimal solution can be reduced to the \textit{Single Source Shortest Path} problem over it.

$ \cal G $ is built as follows: a posting list can be treated as a list containing only positive integers $ L[0,m] $, where $ m $ is the number of postings.
Then each integer is represented by a vertex, plus a dummy vertex marking the end of the sequence.
$ \cal G $ is complete with $ O(m^2) $ edges, each edge is an exact correspondence of a partition in the $ L $.
The weight of an edge in the $ \cal G $ is equal to the cost in bits consumed by the partition.
The problem of fully partitioning $ L $ is converted to finding a path $\pi \in \Pi $ in $\mathcal{G}$, with its total edge weights $ \omega(\pi) $ minimized.

It has been commonly used in many list-oriented encoders \cite{song2016optimizing}.
These encoders usually partition a posting list into chunks of different lengths aligning to its local clusters.
Earlier ones like Fixed Binary \cite{anh2004index} and AFOR use a simple greedy search without backtracking.
Silvestri and Venturini introduce dynamic programming recurrence for VSEncoding \cite{silvestri2010vsencoding} while Guiseppe et al. resort to a more efficient approximation algorithm.
\begin{figure}
	\centering
	\subfloat[DAG for SSSP problem, edges have different lengths and are attatched with one space cost.]{
		\label{fig: sssp}
		\includegraphics[width=1.0\linewidth]{sssp}
	}\\
	\subfloat[DAG for RCSP, edges have fixed length (128) and are attached with space cost and time weight. ]{
		\label{fig: rcsp}
		\includegraphics[width=1.0\linewidth]{rcsp}
	}
	
	\caption{Shortest path finding on two kinds of DAGs, we use red lines to illustrate how to build one feasible path from start to end.}
	\label{fig: DAG}
\end{figure}

However, our version of DAG $ \cal G $ has quite different definitions.
The above methods are designed to pursue space-optimal partitioning at all costs, without considering the decompression speed.
Our goal is to find one pareto-optimal solution under predefined space/time budget, in the new graph, each edge will have attached two weights: a \textit{time weight}, that accounts for the time to decompress a block, and a \textit{space cost}, that accounts for the number of bits needed to store a block associated to that edge.
The most notable contrast in our $ \cal G $ lies in the blocks of constant-size, which determine edge have fixed start and end points.
Between these points, there can be multiple edges to choose from due to the diversity of encoders.
So the total number of edges are exactly $ k \cdot \lceil \dfrac{m}{128} \rceil $ rather than $ O(m^2) $.
For every path $ \pi $ from node 1 to node $ m $, we use $ s(\pi) = \sum_{i=1}^{l}\sum_{j=1}^{k}x_{i,j}s_{i,j} $ to represent the whole compressed size, where $ l= \lceil \dfrac{m}{128} \rceil$, $ x_{i,j} $ represents the choice of the encoder for block $ i $, and is 1 if the chosen encoder is $ j $ and 0 otherwise, $ s_{i,j} $ denotes the space cost of block $ i $ using encoder $ \mathcal{E}_j $.
Accordingly, the total decompression time is set to be $ t(\pi) = \sum_{i=1}^{l}\sum_{j=1}^{k}t_{i,j} $.
Thus we are able to rephrase the bicriteria compression problem into a Resource Constrained Shortest Path problem (RCSP) over the double weighted $ \cal G $, namely finding a path $ \pi $ whose decompression time $ s(\pi) < S $ while its compressed size $ t(\pi) $ minimized.
Symmetrically, we can also exchange the role of space/time resources, which asks minimal $ s(\pi) $ with $ t(\pi) < T $.
For the sake of convenience, we will consider the first formulation only in the rest of the paper. 
An example of two different DAGs is shown in Figure~\ref{fig: DAG}.
%The common foundation for the abovementioned methods is to recast the integer sequence $\mathcal{S} \left[ 0,n \right]$ to a particular DAG $\mathcal{G}$, each integer is represented by a vertex, plus a dummy vertex marking the end of the sequence.
%The graph $\mathcal{G}$ is complete, which means that for any $i$ and $j$ with $i < j \leqslant n$, there exists an edge connecting $v_{i}$ and $v_{j}$, denoted as $\left( v_{i}, v_{j} \right)$.
%In fact, the edge is an exact correspondence of a partition in the sequence $\mathcal{S}\left[i,j \right] $, the problem of fully partitioning $\mathcal{S}$ is converted to finding a path $\pi$ in $\mathcal{G}$, for instance, $\pi=\left( v_{0}, v_{i_{1}} \right) \left( v_{i_{1}}, v_{i_{2}} \right)\ldots\left( v_{i_{k-1}}, v_{n} \right)$ with $k$ edges corresponds to the partition $\mathcal{S}\left[0,i_{1}-1 \right]\mathcal{S}\left[i_{1},i_{2}-1 \right] \ldots\mathcal{S}\left[i_{k-1},n-1\right] $ of $k$ blocks.
%The weight of an edge in the graph is equal to the cost in bits consumed by the partition. Thus, the problem of optimally partitioning a sequence is reduced to the problem of Single-Source Shortest Path(SSSP) Labeling, as shown in Fig.~\ref{fig:sssp}.
%An intuitive way to solve this is to firstly set the cost of each vertex in $\mathcal{G}$ to $+\infty$, then an iteration starts from the left vertex to the rightmost, when it comes to a vertex $v_{j}$ with $0\leqslant j < n$, a subproblem of find the optimal path from $v_{j}$ to $v_{n}$ shows up, assuming the optimal path from $v_{0}$ to $v_{j}$ has been correctly computed.
%Each edge $\left( v_{j},v'\right)$ outgoing from $v_{j}$ will be assessed and cost of vertex $v'$ is updated if it becomes smaller. As can be seen, the time complexity of this algorithm is proportional to the number of edges in $\mathcal{G}$.

\section{Modeling the Problem}\label{sec: model}
%从这里想起来应该是把所有的posting list连接起来才对
In order to formulate the bicriteria compression problem, we need firstly model the space occupancy and decompression time of each block.
As stated in the Secion~\ref{sec: preliminaries}, we assume each posting list is split into blocks of fixed-size with a total number of $ n $, $ s_{i,j} $ is used to denote the compressed size of the \textit{i}th block encoded by the \textit{j}th encoder.
It can be obtained by compressing blocks using all the encoders before processing.
We follow \cite{ottaviano2015optimal} by setting $ T $ as the expected time to process queries from a given query set $ \cal Q $, and adopting the same model to predict the decompression time $ t_{i,j} $.
Then $ T $ is closely related to query processing strategy used and the distribution in $ \cal Q $.
We set $ t(\pi) = \sum_{i=1}^{n}x_{i,j}\hat{t}_{i,j} $, where $ \hat{t}_{i,j}=f_i t_{i,j} $, $ f_i $ is the number of times the \textit{i}th block was decoded while processing $ \cal Q $.
Also, $ f_i $ can be set to 1 if to measure the thorough decompression time of the index.

Now we can formulate our problem using Lagrangian relaxation into the famous RCSP as follows:
\begin{align}
 & \min_{\pi \in \Pi} t(\pi) \notag \\
 \text{s.t.}\quad & s(\pi) \leqslant \cal S \label{equ: RCSP}
\end{align}
By setting $ f(\pi) = t(\pi) $ and $ g(\pi) = s(\pi)- \mathcal{S} $, we may rewrite the Equation~\eqref{equ: RCSP} as
\begin{align*}
& \min_{\pi \in \Pi} f(\pi) \\
\text{s.t.}\quad & g(\pi) \leqslant 0
\end{align*}

A brute-force solution to this problem would cost $ O(n^k) $ time and $ O(nk) $ space.
By adopting a dynamic programming approach proposed in \cite{lawler2001combinatorial}, both time and space can be reduced to $ \Theta(n\mathcal{S}) $, which is $ O(m^{2}\log m) $ at its worst.
Unfortunately this bound is still unacceptable is practice.

To efficiently solve the problem we introduce the Lagrangian function to relax the constraint $ g(\pi) \leqslant 0 $:
\begin{align}
L(\mu,\pi) = f(\pi)+\mu g(\pi) \label{equ: L(u,pi)} \\ 
\intertext{and let}
L(\mu) = \min_{\pi \in \Pi} L(\mu,\pi) \label{equ: L(u)}
\end{align}
\noindent $ \mu $ is the Lagrangian multiplier following $ \mu \geqslant 0 $.
It has been shown that this problem can be solved in linear time by solving this dual problem \cite{handler1980dual}.
Let $ \pi $ be the path we find, and $ \pi^\star $ be the optimal path of the RCSP problem.
At the end of Section~\ref{sec: our alg}, we will have proved the following theorem:
\newtheorem{theorem}{Theorem}
\begin{theorem}
	$ \pi $ can be computed in $ O(\log n) $ time and $ O(n) $ time with its time cost $ t(\pi) \leqslant f(\pi^\star) + t_{max} $ and space cost $ s(\pi) \leqslant S + 2s_{max} $.
\end{theorem}
Here we use $ s_{max} $ and $ t_{max} $ to denote the maximum space/time cost of one block.

\section{Our approximation Algorithm}\label{sec: our alg}
In this section we turn to the algorithm which solves the Lagrangian relaxation problem.
Recall the algorithm in \cite{ottaviano2015optimal}, which first sorts all the possible encodings of each block across the whole index, then adopts a greedy search to scan the sorted list and replace a space-optimal solution with faster encodings until the space budget $ S $ is reached, thus meeting with its additive guarantee.

Our algorithm works in a quite different way, its procedure can also be divided into two phase.
In the first phase, we adopt the cutting-plane algorithm to solve the Lagrangian dual problem introduced in \cite{handler1980dual}.
This algorithm starts from two extreme paths (say, $ \pi(0) $ and $ \pi(\infty) $) and fuses them to get a new path $ \pi' $, then recursively fuses $ \pi' $ with one of the former path until
a lower bound of $ L(\mu,\pi) $ is reached.
Thus we get an instantiation of $ \mu^\star $ corresponding to the maximal value of $ L(\mu) $, and a pair of paths $ (\pi_l,\pi_r) $ which are optimal for $ L(\mu^\star) $ and such that $ g(\pi_l) \geqslant 0 $ and $ g(\pi_r) \leqslant 0 $.

In case one of the two path has its space cost $ s(\pi_r) = S $, then its time cost is the optimal one of $ t(\pi^\star) $.
Unfortunately, this rarely occurs in practice and in most cases we have to tackle duality gap between $ \pi_l $ and $ \pi_r $.
In the second phase, we will execute a sequential exchange of edges from both paths.
After that, we will be able to get an optimal path whose time and space costs better approximate the budget.

\subsection{Solving the Dual Problem}
This phase mainly contributes to the fusion of two extreme paths, one of which is space-optimal while the other one is time-optimal, thus making the fused path converges gradually to the given range through an finite iteration.
The key step stays in generating the optimal path $ \pi' $ for $ \mu $ in each iteration.

Notice that in Equation~\eqref{equ: L(u,pi)} we have $ \mu \geqslant 0 $ and $ g(\pi) \leqslant 0 $, thus $ L(\mu,\pi) $ representing $ f(\pi) $ adding a negative number.
As we are trying to find a minimal $ f(\pi) $, we get the following equation:
\begin{equation}
L(\mu) \leqslant f(\pi^\star) \label{equ: maximum}
\end{equation}
\noindent For every $ \mu \geqslant 0 $.
Therefore, an best lower bound for the optimal path in RCSP can be found by solving its dual problem defined as 
\begin{equation}
L^\star = L(\mu^\star) = \max_{\mu \geqslant 0} L(\mu) =\max_{\mu \geqslant 0} \min_{\pi \in \Pi} \left[ f(\pi) + \mu g(\pi) \right]
\end{equation}

Now the problem has been converted from searching for an optimal path to a linear programming calculating maximal $ L(\mu) $, where each path defines a feasible region.
And our goal is find out the vertex which maximize $ L(\mu) $ from the convex hull defined by the intersection of paths.

However, it is impossible to figure out the convex hull using traditional simplex method since there can be $ O(n^k) $ paths.
Actually we only need a small part of them to heuristically pruning out unfeasible paths and determining the maximal $ L(\mu) $.
\begin{figure}
	\centering
	\includegraphics[width=1.0\linewidth]{intersection}
	\caption{Each path $ \pi $ corresponds to a line in the $ (\mu, L) $ space, we only keep one pair of lines formed by $ (\pi_l, \pi_r) $ and their intersection point is the maximum of current $ L(\mu') $. The iteration tries to find the minimal $ L(\mu') $ as lower bound, the upper bound is the $ L $-intercept of the last $ \pi_r $.}
	\label{fig: intersection}
\end{figure}

The procedure can be best explained geometrically.
Each path in $ \Pi $ may be associated in $ (\mu, L)（$ space with a line of the form $ L = f(\pi) + \mu g(\pi) $, where $ f(\pi) $ is the intercept along the L-axis and $ g(\pi) $ is the slope of the line.
Feasible paths have a non-positive slope (sice $ g(\pi) \leqslant 0 $) and feasible paths have a positive slope.
As mentioned above, we only keep a pair of paths in hand and recursively intersect them to determine the point $ (\mu',L') $.
Note $ L(\mu) $ represents the lower envelope of the all the paths generated so far, so it will consist of two lower segments of the corresponding paths and the point of intersection will maximize $ L(\mu) $.
If there exists another path $ L(\mu') $ ( generated using $ \mu' $) which further tightens the lower envelope $ L' $, then we can have the maximum value of $ L(\mu) $ more close to $ L^\star $ from above.
So the whole procedure works to find a minimal maximum of $ L(\mu) $ since $ L(\mu') \geqslant L(\mu^\star) $.
The iteration stops once $ L(\mu') $ cannot be reduced or we find a path $ \pi^\circ \in \Pi $ with $ g(\pi) = 0 $ and $ L(\mu') = L(\mu', \pi^\circ) < L' $, then the point $ (\mu', L(\mu')) $ represents the optimal solution for the dual problem.

For a given $ \mu $, a path is called $ \mu $-\textit{optimal} if its Lagrangian cost $ L(\pi,\mu) $ equals to the value of $ L(\mu) $ in equation~\eqref{equ: L(u)}.
Consider the initial step, which is to find two extreme paths with respect to function $ f(\pi) $ and $ g(\pi) $ respectively.
Since $ \mu \in [0,+\infty) $ and for each path $ L(\mu) $ represents a monotonic line in $ (\mu,L) $ space.
Naturally, the extreme path exists at $ \mu = 0 $ and $ \mu=\infty $.
Let $ \pi(\mu) $ denote an $ \mu $-\textit{optimal} path associating with $ \mu $,
$ \pi(0) $ corresponds to $ L(0) = \min\{f(\pi)|\pi \in \Pi\} $ which is the minimum of $ f(\pi) $, if $ g(\pi) \leqslant 0 $ then $ \pi(0) $ is also optimal for RCSP.
And $ \pi(\infty) $ corresponds to the minimum of $ g(\pi) $, if $ g(\pi(\infty)) > 0 $, then there will be no solution for RCSP.

It is easy to prove that $ \pi(0) $ and $ \pi(\infty) $ are exactly time-optimal and space-optimal paths in our scenario.
We start by setting $ (\pi(0),\pi(\infty)) $ as $ (\pi_l,\pi_r) $, each time we intersect them, we get the point $ \mu' = (f_l - f_r)/(g_r-g_l) $ and $ L'= f_l + \mu' g_l $.
Then a new path $ \pi' $ is generated as $ L(\mu') $, if $ g(\pi') < 0 $, $ \pi_r $ will replaced by $ \pi' $ and vice versa.
The procedure terminates when $ L(u') = L' $ or $ g(\pi') = 0 $.
It has been shown the number of iterations is $ O(\log(n $$ s_{costs} $$ t_{costs})) $ \cite{mehlhorn2000resource}, where $ s_{costs} $ and $ t_{costs} $ represent the block space/time cost in integers, in our scenario these two are negligible constant since block size is fixed to 128.
This procedure is illustrated in Figure~\ref{fig: intersection}.

It remains to describe how to generate an $ \mu $-\textit{optimal} path $ \pi' $ given the point of intersection $ (\mu',L') $.
A brute force search along the whole graph for $ \pi' $ by evaluating its Lagrangian cost is impossible,
even our version of  $ \cal G $ has already reduced its number of edges from $ O(n^2) $ to $ O(nk) $ by fixing the edge length.
However, fixing the size of blocks indeed bring one significant advantage that blocks are kept isolated from each other.
Thus we can break this problem into numerous subproblems of selecting proper encodings for each block, which minimize its Lagrangian cost given any $ \mu \geqslant 0 $.
Finally we can sum their space/time costs together without collision.
Namely,
\begin{equation*}
L(\mu,\pi) = L(\sum_{i=1}^{n} e_i, \mu)=\sum_{i=1}^{n}L(e_i,\mu)
\end{equation*}
Here $ e_i $ denotes edges in path $ \pi $.

To find the appropriate encodings we need first recall the dominance criteria proposed in \cite{ottaviano2015optimal}: a) Let \textit{p} and \textit{q} be two items in the same class \textit{i}.
Item \textit{q} is dominated by item \textit{p} if $ s_{i,p} \leqslant s_{i,q} $ and $ \hat{t}_{i,p} < \hat{t}_{i,q} $;
b) Let \textit{p}, \textit{q} and \textit{r} be three items in the same class such that $ s_{i,p} < s_{i,q} < s_{i,r} $ and $ \hat{t}_{i,p} \geqslant \hat{t}_{i,q} \geqslant \hat{t}_{i,r} $.
Item \textit{q} is dominated by \textit{p} and \textit{r} if the following condition holds $ \frac{\hat{t}_{i,p} - \hat{t}_{i,q}}{s_{i,p} - s_{i,q}} \geqslant \frac{\hat{t}_{i,q} - \hat{t}_{i,r}}{s_{i,q} - s_{i,r}} $.
After applying these criteria, for each block we have the dominated items pruned from $ \cal G $ and the rest items sorted in ascending order by their space cost.
Also a time/space gain ratio between adjacent items is define as $ \lambda_{i,j} = \frac{\hat{t}_{i,j} - \hat{t}_{i,j-1}}{s_{i,j-1} - s_{i,j}} $.

Let $ L_{i,j} $ denote the Lagrangian cost of the \textit{i}th block encoded by the \textit{j}th encoder.
Our key observation is that the minimal Lagrangian cost of each block can be found by finding the smallest $ \lambda_{i,j} $ that approximates $ \mu $ from above.
As shown in the following equation:
\begin{equation}
\begin{split}
L_{i,j} - L_{i,j-1} &= f(e_{i,j}) - f(e_{i,j-1}) + \mu (g(e_{i,j}) - g(e_{i,j-1})) \\
	&= \hat{t}_{i,j} - \hat{t}_{i,j-1} + \mu (s_{i,j} - s_{i,j-1}) \\
	&= \lambda_{i,j}(s_{i,j-1} - s_{i,j}) + \mu (s_{i,j} - s_{i,j-1}) \\
	&= (s_{i,j} - s_{i,j-1})(\mu - \lambda_{i,j})
\end{split}
\end{equation}
Since $ s_{i,j} > s_{i,j-1} $, encodings with $ \lambda_{i,j} \geqslant \mu $ will reduce the cost of current block,
and the minimal $ L_i $ is exactly $\min\{ \lambda_{i,j}| \lambda_{i,j} \geqslant \mu \} $.

The set of $ \lambda $s is sorted once and accessed many times, this can be done in constant time with simple operations since there are only $ k $ encoders to choose.
Here, we emphasize that the above operations are carried out within a block level rather than the whole index.
To generate a 1-\textit{n} path would obviously cost $ O(nk) $ time and space.
Recall the number of iterations previously mentioned, the dual problem can be solved in $ O(\tilde{n}\log(ns_{costs}t_{costs})) $ time and $ O(\tilde{n}) $ space, where $ \tilde{n} = nk $ is the $ \cal G $'s size.

\subsection{Closing the Gap}
Upon termination of the iteration we have two paths $ (\pi_l,$ $\pi_r) $ which specify a lower bound and an upper bound of the minimal value $ f(\pi^\star) $.
From Equation~\eqref{equ: maximum}, we know the lower bound is the intersection point, namely $ LB = L(\mu^\star) $; also, any path $ \pi $ with $ g(\pi) \leqslant 0 $ generated during the iteration is a feasible solution for RCSP, and the best value for upper bound is $ f(\pi_r) $ from the last path $ \pi_r $.
Thus we have the bound $ L(\mu^\star) \leqslant f(\pi^\star) \leqslant f(\pi_r) $, which is also shown in Figure~\ref{fig: intersection}.

Obviously, if $ L(\mu^\star) = f(\pi_r) $ or any one path in $ (\pi_l,\pi_r) $ satisfies $ g(\pi) = 0 $, there would be no duality gap between solutions to RCSP and its dual problem.
However, this rarely happens in practice.
Therefore we have to deal with duality gap until an optimal path is found for RCSP.

Following \cite{farruggia2014bicriteria}, we reuse the path-swapping algorithm introduced for this task.
The notion of \textit{path swapping} can be explained as follow: given a pair of paths $ (\pi_l,\pi_r) $, we first choose one point $ v $ along the paths arbitrarily as a \textit{pivot}, then we create two new paths $ (\pi_1,\pi_2) $ by exchanging edges before the pivot of $ (\pi_l,\pi_r) $.
In our context, the algorithm is greatly simplified since edges of all paths have fixed start and end points, we do not need to consider cases where edges are split or merged for shared pivot.
It has been shown that any path generated by path-swap is off at most off at most $ t_{max} $ in time and $ s_{max} $ in space from being a $ \mu $-optimal path, $ t_{max} $ and $ s_{max} $ denote maximum time/space cost of one block.
Indeed, the operation is executed on two closely resembled $ \mu $-optimal paths edge-wise.
By moving the pivot from left to right, we are actually substituting path $ \pi_l $ for path $ \pi_r $ edge by edge.
These intermediate paths are naturally bounded by $ \pi_l $ and $ \pi_r $, and there definitely exists a $ \mu $-optimal path whose cost is at most one block-size smaller than these paths.

The block-wise operation also guarantees paths generated using two adjacent pivots differ from each other by at most $ t_{max} $ and $ s_{max} $.
A left-to-right scanning is sufficient to change the Lagrangian cost from $ L(\mu^\star) $ to $ f(\pi_r) $ in a appropriate granularity, and find an optimal solution for $ f(\pi^\star) $ with an additive approximation.
During the procedure, we only need an accumulator to store the current pivot and time/space cost, thus the path-swap algorithm can be done in $ O(n) $ time and $ O(1) $ space.

\section{Experiments}\label{sec: experiments}
In order to provide an experimental evaluation of the proposed compression technique, we have implemented our algorithm, which we call \textit{bicriteria}, in C++.
All the codes related with experiments are available at GitHub\footnote{Anonymized for review}.
In special, we would like to compare its efficiency and compression effectiveness with the original method called \textit{hybrid} in \cite{ottaviano2015optimal}, so we retain most configurations of their experiments to avoid interference from other factors.

\subsection{Experimental Setup}
We choose the same encoder set $ \cal E $ used in \cite{ottaviano2015optimal}, namely Interpolative, PFD(\textit{h}) and Varint-G8IU.
Here \textit{h} stands for exception bit length, and different \textit{h} are treated as different encoders.
Blocks with less than 128 postings are always compressed by Interpolative.

We use the posting lists extracted from the following two collections: \textbf{TREC GOV2} and \textbf{Common Crawl}.
TREC GOV2 is a crawl of the \textbf{.gov} sites used in TREC 2004 Terabyte Track, which consists of 25.2 million documents and 15.3 million terms.
Common Crawl is a corpus of web crawl data composed of over 5 billion web pages over last 7 years, and it keeps growing until now.
The crawl data is stored using WARC 1.0 format with about 541 TB in size.
This whole collection is freely available on Amazon S3\footnote{\url{http://commoncrawl.org/}}. 


Since the Common Crawl corpus is too large to fit in one machine, we only extract a small part of the data crawled in February 2016.
Documents from these two collections are prepared by applying Porter stemmer after removing stopwords, then the docids are reordered by the lexicographic order of URLs.
Table~\ref{tab: collection statistics} compares these two collections using some basic statistics.

\begin{table}
	\centering
	\caption{Collection statistics for GOV2 and Common Crawl}
	\renewcommand{\arraystretch}{1.0}
	\begin{tabular}{l*{2}{r}}
		\toprule
		& \multicolumn{1}{c}{GOV2} & \multicolumn{1}{c}{Common Crawl} \\
		\midrule
		Documents & 25,203,921 & 17,157,948 \\
		Terms & 15,324,160 & 51,990,893 \\
		Pointers & 4,655,778,182 & 9,118,361,737 \\
		Document Length(avg.) & 645.25 & 2151.83 \\
		Posting List Length(avg.) & 303.82 & 173.38 \\
		\bottomrule
		\label{tab: collection statistics}
	\end{tabular}
\end{table}

We can find the trend that document length grows with age, making documents involved in more posting lists.
However, Common Crawl is less organized than GOV2, since the former is collected from the whole web while the latter is limited in \texttt{.gov} domain, then terms in GOV2 are more repetitive and posting list length is larger.
Next we will reveal how these differences affect performances of index.

All the implementations are carried out on a PC server with an 8 core Intel(r) Xeon(r) E5620 processor running at 2.40 GHz, with 32GB of RAM and 12,288KB of cache. Our algorithm is compiled with GCC 4.8.1 with -O3 optimizations. In all our runs, executions are reported as the mean of 4 consecutive replications.

The query set $ \cal Q $ we use is a realistic query log released by AOL \cite{pass2006picture}.
It contains of about 20 million Web queries collected from 650 thousand users over three months.
We first execute all the queries using processing strategies that will be used in experiment to count up the access statistics $ f_i $ of each block.
Then we randomly pick 10000 queries, which contains at least 2 terms present in the posting lists, to examine query efficiency of the built index.
Before performing queries, the whole inverted index is completely loaded into main memory, in order to avoid the confusion caused by disk I/O.

\subsection{Indexing Performance}
\paragraph*{Construction time}
We have analyzed our algorithm in detail in the previous section.
Its complexity is $ O(\tilde{n} $$ \log(n $ $ s_{costs} $$ t_{costs})) $ in time and $ O(\tilde{n}) $ in space, almost equaled by \textit{hybrid} which is $ O(\tilde{n} $$ \log(\tilde{n})) $ in time and $ O(\tilde{n}) $ in space.
Both \textit{bicriteria} and \textit{hybrid} concatenate all the posting lists into one to arrange encodings of each block, and are delayed by one time-consuming phase that cannot be paralleled, for \textit{bicriteria} is the iteration when solving the dual problem, for \textit{hybrid} is the sorting of $ \lambda_{i,j} $.

However, in practice \textit{bicriteria} compresses more efficiently than \textit{hybrid}.
The average time used on GOV2 is 22 minutes, and on Common Crawl is 50 minutes.
In contrast \textit{hybrid} costs 30 and 71 minutes, respectively.
Note that time complexity of the iteration is measured in the worst case scenario, in our experiment the algorithm converges rapidly after few iterations.
We can further improve the compression efficiency by amortizing the expected budget over each posting list rather than concatenating all the posting lists together, at the cost of losing the approximation guarantee.
However, we can calculate the optimal path and compress blocks for each posting list separately, namely the whole phases of \textit{bicriteria} can be accelerated by parallel.
\begin{figure}
	\centering
	\subfloat[linearly varying space budget $ S $ from 0 to 1 by 0.1]{
		\label{fig: cc_S}
		\includegraphics[width=1.0\linewidth]{cc_S}
	}\\
	\subfloat[linearly varying time budget $ T $ from 1 to 0 by 0.1]{
		\label{fig: cc_T}
		\includegraphics[width=1.0\linewidth]{cc_T}
	}
	\caption{Percentage of encoders used when compressing blocks under different space/time budgets for Common Crawl, the abscissa represents compressed size in GB.}
	\label{fig: percentage}
\end{figure}
\paragraph*{Trading off space and time}
In order to demonstrate \textit{bicriteria} has the ability to flexibly trade off compressed size and query processing time, we perform two series of experiments by specifying space and time as budget respectively.
Space budget is obviously the compressed size, we set the time budget as the sum of expected query processing time for all the queries in $ \cal Q $.
The budget is represented using a compression level $ l = \frac{B-B_0}{B_\infty-B_0} $, where $ (B_0,B_\infty) $ is the minimum and maximum space/time the index can achieve.
For example, if $ B = S $, then $ B_0 $ is the most succinct space optimal compression with $ l = 0 $, while $ B_\infty $ is the fastest compression with $ l = 1 $.

As shown in Figure~\ref{fig: cc_S}, by linearly relaxing the budget in space, \textit{bicriteria} trends to choose more fast encoders.
At first more than 90\% blocks are compressed using Interpolative, then more PFD-compressed blocks are involved and last nearly all of them are compressed using Varint-G8IU.
The result agrees with that of \cite{ottaviano2015optimal}, however, their space-time curve is more smoothing because when the space budget changes, \textit{hybrid} executes a sequential scan to replace encoders right starting from the last block it stopped.
\textit{Bicriteria} works in a more \textbf{balanced} way, the budget is amortized to all the blocks since it relies on path finding rather than sorting, while \textit{hybrid} is a \textbf{greedy} algorithm, it always prefer blocks which are more frequently accessed.

Figure~\ref{fig: cc_T} shows how space changes when we linearly tightens time budget ($ T $ varies from 1 to 0).
It is quite different from \ref{fig: cc_S} as Interpolative is the preferred choice in most cases, even when $ l = 0.5 $ there are still nearly 50\% blocks using Interpolative.
This is caused by the fact that the average length of posting list in Common Crawl is 173.38, large number of partial blocks lead to the use of Interpolative.
Also, Interpolative is much slower than other encoders, a small drop of its usage indeed leads to a much reduce in expected query processing time.
Note that this kind of reduce is a balanced result, we have to admit that it does not work well most of the time in real scenario.
However, we can use it to find the proper point where the average decoding time is sharply reduced while its space occupancy keeps low, for example, we can choose $ l=0.5 $ here.

Table~\ref{tab: space and time} reports the indexing performances of \textit{bicriteria} under various budgets in comparison with \textit{hybrid}.
Results are classified by ``method@[\textit{compression level}][\textit{budget type}]", space denotes the index size in gigabytes and time denotes the expected time cost in nanoseconds of decoding one 128-sized block.
The expected time cost is obtained using $ \sum_{i=1}^{n}$ $x_{i,j}\hat{t}_{i,j} / \sum_{i=1}^{n}f_{i} $, taking into account of the impact of frequently accessed blocks.
\begin{table*}
	\centering
	\caption{Comparison of construction time, and average bits per element of each componentmmmmmmmmmm}
	\renewcommand{\arraystretch}{1.0}
	\setlength\tabcolsep{6pt}
	\begin{tabular}{@{}l*{3}{l*{2}{r}}}
		\toprule
		\multirow{2}*{Dataset} & \multirow{2}*{Method} & \multicolumn{1}{c}{Space} & \multicolumn{1}{c}{Time} & \multirow{2}*{Method} & \multicolumn{1}{c}{Space} & \multicolumn{1}{c}{Time} &\multirow{2}*{Method} & \multicolumn{1}{c}{Space} & \multicolumn{1}{c}{Time} \\
		& &\multicolumn{1}{c}{\textbf{GB}} & \multicolumn{1}{c}{\textbf{ns}} & & \multicolumn{1}{c}{\textbf{GB}} & \multicolumn{1}{c}{\textbf{ns}} & & \multicolumn{1}{c}{\textbf{GB}} & \multicolumn{1}{c}{\textbf{ns}} \\
		
		\cmidrule(r){1-1} \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(l){8-10}
		
		\multirow{11}*{GOV2} 
		& bicriteria@\textbf{0S}   & 3.05 & 3026 & hybrid@\textbf{0S}   & 3.05 & 3026 & bicriteria@\textbf{0T}   & 5.43 & 124 \\	
		& bicriteria@\textbf{0.1S} & 3.28 & 1539 & hybrid@\textbf{0.1S} & 3.28 & 832  & bicriteria@\textbf{0.1T} & 3.67 & 413 \\
		& bicriteria@\textbf{0.2S} & 3.53 & 727  & hybrid@\textbf{0.2S} & 3.53 & 389  & bicriteria@\textbf{0.2T} & 3.55 & 700 \\
		& bicriteria@\textbf{0.3S} & 3.77 & 297  & hybrid@\textbf{0.3S} & 3.77 & 203  & bicriteria@\textbf{0.3T} & 3.45 & 990 \\
		& bicriteria@\textbf{0.4S} & 4.00 & 202  & hybrid@\textbf{0.4S} & 4.00 & 153  & bicriteria@\textbf{0.4T} & 3.37 & 1278 \\
		& bicriteria@\textbf{0.5S} & 4.24 & 165  & hybrid@\textbf{0.5S} & 4.24 & 136  & bicriteria@\textbf{0.5T} & 3.30 & 1566 \\
		& bicriteria@\textbf{0.6S} & 4.48 & 145  & hybrid@\textbf{0.6S} & 4.48 & 129  & bicriteria@\textbf{0.6T} & 3.23 & 1857 \\
		& bicriteria@\textbf{0.7S} & 4.72 & 135  & hybrid@\textbf{0.7S} & 4.72 & 126  & bicriteria@\textbf{0.7T} & 3.18 & 2147 \\
		& bicriteria@\textbf{0.8S} & 4.95 & 129  & hybrid@\textbf{0.8S} & 4.95 & 125  & bicriteria@\textbf{0.8T} & 3.12 & 2430 \\
		& bicriteria@\textbf{0.9S} & 5.19 & 126  & hybrid@\textbf{0.9S} & 5.19 & 125  & bicriteria@\textbf{0.9T} & 3.08 & 2719 \\
		& bicriteria@\textbf{1S}   & 5.43 & 124  & hybrid@\textbf{1S}   & 5.43 & 124  & bicriteria@\textbf{1T}   & 3.05 & 3026 \\
		\midrule
		\multirow{11}*{Common Crawl} 
		& bicriteria@\textbf{0S}   & 9.38  & 3559 & hybrid@\textbf{0S}   & 9.38  & 3559 & bicriteria@\textbf{0T}   & 16.14  & 124 \\	
		& bicriteria@\textbf{0.1S} & 10.06 & 1892 & hybrid@\textbf{0.1S} & 10.06 & 867  & bicriteria@\textbf{0.1T} & 11.48  & 466 \\
		& bicriteria@\textbf{0.2S} & 10.73 & 828  & hybrid@\textbf{0.2S} & 10.73 & 348  & bicriteria@\textbf{0.2T} & 10.98  & 854 \\
		& bicriteria@\textbf{0.3S} & 11.41 & 354  & hybrid@\textbf{0.3S} & 11.41 & 185  & bicriteria@\textbf{0.3T} & 10.69  & 1151 \\
		& bicriteria@\textbf{0.4S} & 12.09 & 233  & hybrid@\textbf{0.4S} & 12.09 & 144  & bicriteria@\textbf{0.4T} & 10.43  & 1493 \\
		& bicriteria@\textbf{0.5S} & 12.76 & 182  & hybrid@\textbf{0.5S} & 12.76 & 131  & bicriteria@\textbf{0.5T} & 10.20  & 1933 \\
		& bicriteria@\textbf{0.6S} & 13.44 & 155  & hybrid@\textbf{0.6S} & 13.44 & 127  & bicriteria@\textbf{0.6T} & 9.98   & 2180 \\
		& bicriteria@\textbf{0.7S} & 14.11 & 139  & hybrid@\textbf{0.7S} & 14.11 & 126  & bicriteria@\textbf{0.7T} & 9.80   & 2522 \\
		& bicriteria@\textbf{0.8S} & 14.79 & 131  & hybrid@\textbf{0.8S} & 14.79 & 125  & bicriteria@\textbf{0.8T} & 9.64   & 2839 \\
		& bicriteria@\textbf{0.9S} & 15.46 & 126  & hybrid@\textbf{0.9S} & 15.46 & 124  & bicriteria@\textbf{0.9T} & 9.50   & 3207 \\
		& bicriteria@\textbf{1S}   & 16.14 & 124  & hybrid@\textbf{1S}   & 16.14 & 124  & bicriteria@\textbf{1T}   & 9.38   & 3559 \\

		\bottomrule
		\label{tab: space and time}
	\end{tabular}
\end{table*}

Generally, \textit{bicriteria} is able to find the exact solution under different budgets.
As shown in the first two columns of the table, \textit{bicriteria} achieve the same space as that of \textit{hybrid} on both GOV2 and Common Crawl.
However, the expected time costs they got are quite different from each other.
We can see that \textit{hybrid} always have smaller cost than \textit{bicriteria}, except under two extreme cases where they all adopt the same optimal solution.
Also, the cost got by \textit{hybrid} drops sharply as the space budget grows while \textit{bicriteria} changes relatively slow.
Note this phenomenon is caused by the internal mechanism of two different methods.
Given extra space, \textit{hybrid} will update the most frequently accessed blocks of the remaining ones, this will have a direct effect on the decrease of expected time.
However, \textit{bicriteria} tries to share the space to as more blocks as possible, because when we generate a path in the first phase, the edge-selection for each block is independent.
Thus resulting a larger average cost than \textit{hybrid}.

The result is also consistent with the percentage shown in Figure~\ref{fig: cc_S}.
Compared with \textit{hybrid}, \textit{bicriteria} uses more Interpolative at first.
And in the middle of the figure, PFD accounts for a larger proportion than that of \textit{hybrid}.
In a word, our method focuses on improving the average performance of query processing while \textit{hybrid} focuses on guaranteeing performance of the most frequently used queries.

The third column reports result of \textit{bicriteria} using time as budget.
The space slowly grows as we relax the time budget from 1 to 0.1, because 10\% growth in space can sharply reduce the expected time cost.
Also, the expected time cost can hardly be satisfied since it is an averaged value, from Figure~\ref{fig: cc_T} we can see most of the blocks are compressed using Interpolative.
A subdivision between 0T and 0.1T can be used to find more practical tradeoffs, but there needs a careful measurement of the connection between the expected time budget and actual query processing time. 

\subsection{Query Efficiency}

In order to explore the efficiency differences between \textit{bicriteria} and \textit{hybrid}, we adopt three widely-used methods of ranked query to find the top-20 results, Ranked\_AND, WAND and Maxscore.
Due to the limited space, we only report partial result (the space budget grows from 0 to 1 by 0.2) on Common Crawl, the omitted part and result on GOV2 exhibit similar performance except that query time on GOV2 gets an overall decline because of smaller index size.
We also omit performances of single encoders, as our goal is only to prove that \textit{bicriteria} obtains a better average processing time in practice with the help of a more balanced selection of encoders.

As shown in Figure~\ref{fig: query}, we can see that both the methods get similar query efficiency under various scenarios.
Even though Table~\ref{tab: space and time} reports \textit{hybrid} has a smaller expected processing time, practical experiment shows a quite different result, \textit{bicriteria} works better in some cases.
Among all three processing strategies, the most evident performance gap occurs in Ranked\_AND, we can see at smaller budget (0.2S and 0.4S), the average time of \textit{bicriteria} is smaller than \textit{hybrid}.
This is due to the fact that \textit{hybrid} prefers optimizing those frequently accessed blocks inside the whole index, if one posting list has few this kind of blocks or the entire list is rarely accessed, then the processing time will be extended because of too many ``slow" blocks.
However, encoders assigned to blocks using \textit{bicriteria} is more evenly distributed. 
As Ranked\_AND processes queries by traversing all the blocks inside the posting lists, the difference is more apparent.
We can also find gaps caused by WAND and MaxScore are less vulnerable to this problem because these two strategies are more likely to decode those frequently accessed blocks.

Another noticeable phenomenon is performance gaps get blurred when the budget grows larger.
The reasons are twofold.
First, both methods adopt more fast encoders, the different blocks begin to reduce.
Second, limited timing accuracy makes it harder to compare these slight differences.
\begin{figure}
	\centering
	\includegraphics[width=1.0\linewidth]{query}
	\caption{Query time distribution of \textit{bicriteria} and \textit{hybrid} using different processing strategies on Common Crawl, the budget varies from 0 to 1. The average query time is represented by plus sign.}
	\label{fig: query}
\end{figure}

\section{Conclusions and Future Work}
In this paper we have introduced a bicriteria optimization on building hybrid index.
Compared with its original version, our method is also able to yield a flexible space-time tradeoff under a given budget, while preserving the same additive approximation guarantees.
Moreover, it supports regarding either time or space as budget and is possible to be extended using variable-sized blocks.
Our method is more theoretically well-founded, we have given detailed proof of its smaller space and time complexity, also the experiment has demonstrated \textit{bicriteria} gains smaller construction time and average query processing time.

Since our solution only focuses on modeling methodology rather than implementation, we do not consider integrating other encoders into the method.
Also, there needs a better tuned measurement to connect expected budget and practical performance when using time as budget.
Finally, it would be worthwhile to extend \textit{bicriteria} with variable-sized blocks.

\bibliographystyle{abbrv}
\balance
\bibliography{reference}

% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns

\end{document}
